{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5ea0d3-cc1c-41fc-a1c3-896219369a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################## GENERAL IDEA ###################################################################\n",
    "###################################################################################################################################################\n",
    "############################################################### 1. PRE PROCESSING #################################################################\n",
    "###################################################################################################################################################\n",
    "## Create a data frame from csv file, then create two data frames one for each Station types (start, end) for easier data handling. \n",
    "## Making sure data are sorted by datetimes, handling missing data (if any), getting rid of data we do not care about, converting certain columns \n",
    "## to certain data types if needed.\n",
    "## Then, create intervals and calculate values for each interval, handle upper/lower interval problems, and finally creating the final time_serie, \n",
    "## handling missing values if necessary. \n",
    "## All of the above are being conducted inside a function wich returns the final time series. \n",
    "## I did this because I wanted to easily create different time series using different intervals for testing purposes.\n",
    "## At the end of pre processing, we devide the time series into training and testing data. This is also being conducted in a function because\n",
    "## I wanted to easily create different training/testing data analogies for testing purposes.\n",
    "###################################################################################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d9b842-5da1-4e7f-a9cb-80049888c34b",
   "metadata": {},
   "source": [
    "**PREPROCESSING OF DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477e91d4-086c-416f-b81d-e931cd6391c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### Time series creating function, using a file name and an integer representing the interval in hours ###########################\n",
    "\n",
    "# Importing pandas library\n",
    "import pandas as pd\n",
    "\n",
    "def csvDataToTimeSerie(fileName, timeInterInHours):  \n",
    "    # Creating a pandas data frame with the raw data of the csv file\n",
    "    raw_df = pd.read_csv(fileName)\n",
    "\n",
    "    # Creating separate data frames for start and end stations\n",
    "    \n",
    "    raw_start_df = raw_df[['TripId', 'StartTime', 'StartStationId']]\n",
    "    raw_end_df = raw_df[['TripId', 'EndTime', 'EndStationId']]\n",
    "    \n",
    "    # Making sure the data we are going to work on are sorted by timestamp\n",
    "    \n",
    "    raw_start_sorted_df = raw_start_df.sort_values(by='StartTime')\n",
    "    raw_end_sorted_df = raw_end_df.sort_values(by='EndTime')\n",
    "    \n",
    "    # Keeping only data that we want in each data frame (deleting entries where station id is not equal to 1)\n",
    "    # Also resetting the data frames' indexes for better visualization-manipulation of data\n",
    "    \n",
    "    condition = raw_start_sorted_df['StartStationId'] == 1\n",
    "    raw_start_sorted_df = raw_start_sorted_df[condition]\n",
    "    raw_start_sorted_df = raw_start_sorted_df.reset_index(drop='True')\n",
    "    \n",
    "    condition = raw_end_sorted_df['EndStationId'] == 1\n",
    "    raw_end_sorted_df = raw_end_sorted_df[condition]\n",
    "    raw_end_sorted_df = raw_end_sorted_df.reset_index(drop='True')\n",
    "    \n",
    "    # Checking if there are NaN values in the the raw data frames\n",
    "    \n",
    "    start_df_nan_check = raw_start_sorted_df.isna().any()\n",
    "    end_df_nan_check = raw_end_sorted_df.isna().any()\n",
    "    # In this project there are non. If there where, we would have to decide how to deal with the empty spots\n",
    "    \n",
    "    # Converting Time columns to timestamps so that certain functions can be used on them\n",
    "    \n",
    "    raw_start_sorted_df['StartTime'] = pd.to_datetime(raw_start_sorted_df['StartTime'])\n",
    "    raw_end_sorted_df['EndTime'] = pd.to_datetime(raw_end_sorted_df['EndTime'])\n",
    "\n",
    "    # Defining the start and end timestamps of the time series\n",
    "    # First we compare the min and max timestamps from the two data frames\n",
    "    # and we keep the overall min and max timestamps\n",
    "    \n",
    "    start_time_1 = raw_start_sorted_df['StartTime'].min()\n",
    "    start_time_2 = raw_end_sorted_df['EndTime'].min()\n",
    "    if (start_time_1 > start_time_2):\n",
    "        start_time = start_time_2\n",
    "    else:\n",
    "        start_time = start_time_1\n",
    "    \n",
    "    end_time_1 = raw_start_sorted_df['StartTime'].max()\n",
    "    end_time_2 = raw_end_sorted_df['EndTime'].max()\n",
    "    if (end_time_1 > end_time_2):\n",
    "        end_time = end_time_1\n",
    "    else:\n",
    "        end_time = end_time_2\n",
    "    \n",
    "    # Then we round the starting and ending hour (to the floor and ceiling hours respectively)\n",
    "    \n",
    "    start_time = start_time.floor('h')\n",
    "    end_time = end_time.ceil('h')\n",
    "    \n",
    "    # This if will take action only if the timeInterInHours exactly devides 24 (total hours per day)\n",
    "    # Not necessary but if the above is the case, then intervals deviding the total period will not split between different days\n",
    "    \n",
    "    if 24 % timeInterInHours == 0:\n",
    "        start_mod = start_time.hour % timeInterInHours\n",
    "        if start_mod != 0:\n",
    "            start_time -= pd.Timedelta(hours=start_mod)\n",
    "        \n",
    "        end_mod = end_time.hour % timeInterInHours\n",
    "        if end_mod != 0:\n",
    "            end_time += pd.Timedelta(hours=end_mod)\n",
    "    \n",
    "    # Creating the final timestamp values of the time series using a frequency of timeInterInHours hours\n",
    "    # and inserting it into a pandas data frame\n",
    "    # Each timestamp of the time_serie will indicate the demand for the time period [current_timestamp, current_timestamp+Xhours)\n",
    "    \n",
    "    interval = str(timeInterInHours) + 'h'\n",
    "    timestamps = pd.date_range(start=start_time, end=end_time, freq=interval)\n",
    "    \n",
    "    # There is a change that the max timestamp created is less than the end_time of our data.\n",
    "    # If this is the case we add one more interval, so that we will not miss any data when creating the time serie\n",
    "    \n",
    "    if (timestamps[-1] < end_time):\n",
    "        extra_timestamp = timestamps[-1] + pd.Timedelta(hours=timeInterInHours)\n",
    "        timestamps = timestamps.append(pd.DatetimeIndex([extra_timestamp]))\n",
    "    \n",
    "    # Creating the total period for the time serie\n",
    "    \n",
    "    pre_time_serie = pd.DataFrame({'Time': timestamps})\n",
    "    \n",
    "    # Calculating the bicycles taken and returned in each period,\n",
    "    # and then calculating the demand in ech period by subtracting the above\n",
    "    \n",
    "    pre_time_serie['Taken'] = 0\n",
    "    pre_time_serie['Returned'] = 0\n",
    "    \n",
    "    # Inserting the taken bicycle values\n",
    "    i = 0\n",
    "    max_i = raw_start_sorted_df.index.max()\n",
    "    max_index = pre_time_serie.index.max()\n",
    "    for index in range(0, max_index):\n",
    "        count = 0\n",
    "        cur_time = raw_start_sorted_df.loc[i, 'StartTime']\n",
    "        low_time = pre_time_serie.loc[index, 'Time']\n",
    "        high_time = pre_time_serie.loc[index+1, 'Time']\n",
    "        while (cur_time>=low_time and cur_time<high_time):\n",
    "            count += 1\n",
    "            i += 1\n",
    "            if (i>max_i): break\n",
    "            cur_time = raw_start_sorted_df.loc[i, 'StartTime']\n",
    "        pre_time_serie.loc[index, 'Taken'] = count\n",
    "        if (i>max_i): break\n",
    "    \n",
    "    # Inserting the returned bicycle values\n",
    "    i = 0\n",
    "    max_i = raw_end_sorted_df.index.max()\n",
    "    for index in range(0, max_index):\n",
    "        count = 0\n",
    "        cur_time = raw_end_sorted_df.loc[i, 'EndTime']\n",
    "        low_time = pre_time_serie.loc[index, 'Time']\n",
    "        high_time = pre_time_serie.loc[index+1, 'Time']\n",
    "        while (cur_time>=low_time and cur_time<high_time):\n",
    "            count += 1\n",
    "            i += 1\n",
    "            if (i>max_i): break\n",
    "            cur_time = raw_end_sorted_df.loc[i, 'EndTime']\n",
    "        pre_time_serie.loc[index, 'Returned'] = count\n",
    "        if (i>max_i): break\n",
    "    \n",
    "    # Creating the resulting time serie by deleted columns that are not needed,\n",
    "    # deleting the last row, which does not represent a period we want\n",
    "    # and make the timestamps' column the index\n",
    "    # We do not have to handle missing values etc,\n",
    "    # because as we can see from the raw data set, data is spread in any kind day/hour\n",
    "    \n",
    "    # If we had to deal with missing values, then usually one of three methods is chosen:\n",
    "    # 1. Fill with front interval's value with time_serie.Demand.fillna(method = 'bfill') a.k.a \"back filling\"\n",
    "    # 2. Fill with back interval's value with time_serie.Demand.fillna(method = 'ffill') a.k.a \"front filling\"\n",
    "    # 3. Fill with the mean of all time_serie data with time_serie.Demand.fillna(value = time_serie.Demand.mean()) -- NOT good choice for time series\n",
    "    #    because usually we observe a periodicity in data and data are strongly connected with neighbor data\n",
    "    \n",
    "    pre_time_serie['Demand'] = pre_time_serie['Returned'] - pre_time_serie['Taken']\n",
    "    time_series = pre_time_serie.copy()\n",
    "    time_series.drop(columns=['Returned'], inplace=True)\n",
    "    time_series.drop(columns=['Taken'], inplace=True)\n",
    "    time_series.drop(time_series.index[-1], inplace=True)\n",
    "    time_series.set_index('Time', inplace=True)\n",
    "\n",
    "    freq = str(timeInterInHours) + \"h\"\n",
    "    time_series = time_series.asfreq(freq)\n",
    "    return time_series\n",
    "\n",
    "############################################ Function to split the time series into training and testing data #######################################\n",
    "\n",
    "def timeSerieSplit(time_serie, training_perc, testing_perc):\n",
    "    size = int(len(time_serie)*training_perc)\n",
    "    training_data = time_serie.iloc[:size]\n",
    "    testing_data = time_serie.iloc[size:]\n",
    "    return training_data, testing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb39822-0db4-4c0a-b51f-207755ee3ee8",
   "metadata": {},
   "source": [
    "**TIME SERIES CREATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30d5079-3ec0-430e-8b17-96e453a9dd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a time series from the data.csv file's data, with a 2 hour interval and splitting into 70% training, 30% testing data ###\n",
    "\n",
    "time_serie = csvDataToTimeSerie('data.csv', 2)\n",
    "ts, ts_test = timeSerieSplit(time_serie, 0.7, 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7960ab6a-dd73-48e6-a200-034df2777fa9",
   "metadata": {},
   "source": [
    "**TIME SERIES ANALYSIS START**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e438a5e-1b70-4705-b5e1-2f2ef53ae117",
   "metadata": {},
   "source": [
    "**COMPARING OUR TIME SERIES WITH WHITE NOISE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c323a2e5-35c2-4049-a689-7fe35f58011b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the time serie through the data frame\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "ts.Demand.plot(figsize = (20, 8))\n",
    "plt.title(\"Bicycle Demand\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Demand\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# White noise plot, following a normal distribution with mean and std derived from the time series data\n",
    "white_noise = np.random.normal(loc = ts.Demand.mean(), scale = ts.Demand.std(), size = len(ts))\n",
    "ts.loc[:,'white_noise'] = white_noise\n",
    "print(ts)\n",
    "ts.white_noise.plot(figsize = (20, 8))\n",
    "plt.title('White Noise TS')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('White Noise')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22164c8-7394-47ac-a153-e090aa944701",
   "metadata": {},
   "source": [
    "We can see that the bicycle demand time series plot looks a lot like the white noise time series plot.<br>\n",
    "They surely both do not have any trend, having a mean very close to zero. The std also seems to be constant.<br>\n",
    "Regarding to seasonality, in white noise we are sure that there is non there. But in bicycle demand, we see that there could or not exist some daily seasonality. There seems to be some kind of periodicity there, but it does not seem strong. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d238c9-6545-4a45-b7c5-328d49e3fe90",
   "metadata": {},
   "source": [
    "**FINDING TREND**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f743e3e-aa0f-410b-a410-315b2e594719",
   "metadata": {},
   "source": [
    "It is obvious by the previous graph that no trend exists in the time series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd290659-98bf-468b-8e51-1eb945e11625",
   "metadata": {},
   "source": [
    "**FINDING SEASONALITY**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06db75ed-5ca3-476e-ae84-73ffd62c4a6e",
   "metadata": {},
   "source": [
    "From the above graph it seems that there could exist seasonality of 1 day period.\n",
    "We will check this hypothesis using autocorrelation.\n",
    "We can also try to find if there is a correlation in every week's data (84 lags), \n",
    "    but with the limited data we have, this examination could lead us to misleading conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0af05a-49f4-4202-b80f-670d45f43976",
   "metadata": {},
   "source": [
    "**CORRELATION TEST WITH ACF METHOD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df924a6-4f0f-48a1-90db-0964ec7aee85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.graphics.tsaplots as sgt\n",
    "\n",
    "sgt.plot_acf(ts.Demand, lags = 84, zero = False)\n",
    "plt.title(\"ACF Test\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "plt.grid()\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a960d2cb-1d61-49fd-81a6-0ab75e15b4dc",
   "metadata": {},
   "source": [
    "Even though the ACF method does not give strong direct/indirect correlations between present and past values,\n",
    "we can see a pattern in the graph with a frequency of 1 day (12 lags x 2 hour interval).\n",
    "This indicates that MAYBE seasonality exists in the time series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d1179f-8f90-44a7-bc7d-523c0b6ef7bc",
   "metadata": {},
   "source": [
    "**CORRELATION TEST WITH PACF METHOD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ec0f51-3478-453f-82d1-5ab0b0d3706b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgt.plot_pacf(ts.Demand, lags = 84, zero = False, method = 'ols')\n",
    "plt.title(\"PACF Test\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "plt.grid()\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28a782d-8598-4c9f-b69d-fc20734ae6c0",
   "metadata": {},
   "source": [
    "With the use of PACF method we conclude that there is no significant direct correlation between timestamps of any lag size.\n",
    "We could try though to use AR model with 5,6,8,9,10,12,66,83 lags maybe, because we see SOME correlation there.<br>\n",
    "Though direct correlation with 66, 83 lags may be misleading, because we do not have a time series big enough to indicate such a thing with certainty\n",
    "(our data consist of 235 observations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c51e81-58b5-4057-bfab-c25ae0b4417a",
   "metadata": {},
   "source": [
    "**DECOMPOSING SEASONALITY** with the \"naive\" method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec6f9b3-e6c3-451a-b209-aa272622430f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We will use seasonal decomposition to split the seasonality from the time series\n",
    "\n",
    "import statsmodels.tsa.seasonal as stse\n",
    "\n",
    "s_dec_add = stse.seasonal_decompose(ts.Demand)\n",
    "s_dec_add.plot()\n",
    "plt.xticks(rotation = 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b61c2f-737f-4f45-8746-9785e4ac9ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_seasonality = s_dec_add.seasonal\n",
    "ts_seasonality = ts_seasonality.to_frame()\n",
    "ts_seasonality = ts_seasonality.rename(columns={'seasonal': 'season_demand'})\n",
    "ts_no_season = ts.copy()\n",
    "ts_no_season['Demand'] = ts['Demand'] - ts_seasonality['season_demand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbe6caa-0cf5-4b3d-b7c4-665f3dea3abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.Demand.plot()\n",
    "plt.rcParams[\"figure.figsize\"] = (30, 5)\n",
    "plt.title(\"Original Time Series\")\n",
    "plt.show()\n",
    "ts_seasonality.season_demand.plot()\n",
    "plt.title(\"Seasonality in Time Series\")\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "plt.show()\n",
    "ts_no_season.Demand.plot()\n",
    "plt.title(\"Residuals in Time Series (No Seasonality, No Trend)\")\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da8f715-108b-420b-b672-5e197bd0b1df",
   "metadata": {},
   "source": [
    "**ACF and PACF TEST for the time series desomposed from seasonality with \"naive\" method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc27591-69b5-4214-9709-d6c6e0a23747",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgt.plot_acf(ts_no_season.Demand, lags = 84, zero = False)\n",
    "plt.title(\"ACF Test\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "plt.grid()\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 8)\n",
    "plt.show()\n",
    "\n",
    "sgt.plot_pacf(ts_no_season.Demand, lags = 84, zero = False, method = 'ols')\n",
    "plt.title(\"PACF Test\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "plt.grid()\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b2c9b3-3356-4953-b075-844c54ffc51b",
   "metadata": {},
   "source": [
    "In the ACF method we can see that the previous periodicity no longer exist.<br>\n",
    "Also, once again, neither in the ACF nor the PACF do we observe strong correlation between specific lagged timestamps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57fb08e-4e66-4ddc-adf5-0944e9da1eb9",
   "metadata": {},
   "source": [
    "**STATIONARITY TEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca24d5db-1232-49de-90dd-a81b6e6e1606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.tsa.stattools as sts\n",
    "\n",
    "adf_ts = sts.adfuller(ts.Demand)\n",
    "adf_ts_no_season = sts.adfuller(ts_no_season.Demand)\n",
    "\n",
    "print(\"ADF Statistic for the ts: \", adf_ts[0])\n",
    "print(\"p-value for the ts: \", adf_ts[1], \"\\n\")\n",
    "\n",
    "print(\"ADF Statistic for the ts decomposed from seasonal data (with naive method): \", adf_ts_no_season[0])\n",
    "print(\"p-value for the ts decomposed from seasonal data (with naive method): \", adf_ts_no_season[1], \"\\n\")\n",
    "\n",
    "print(\"critical value (for 5%):\", adf_ts[4]['5%'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220886c6-24b0-4d64-9b65-a88673ac0e20",
   "metadata": {},
   "source": [
    "We observe that the ADF_statistic < critical_value and that the possibility of the both ts and ts_no_season being non-stationary is close to 0.\n",
    "It seems safe to declare that both are stationary.\n",
    "\n",
    "Lets try to run the augmented dickey fuller test, without a max limit of 84 lags (1 week)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e1ae0a-b040-46ba-aebe-35f6d043fc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_ts_no_lag_limit = sts.adfuller(ts.Demand, maxlag=84)\n",
    "adf_ts_no_season_no_lag_limit = sts.adfuller(ts_no_season.Demand, maxlag=84)\n",
    "\n",
    "print(\"ADF Statistic for the ts: \", adf_ts_no_lag_limit[0])\n",
    "print(\"p-value for the ts: \", adf_ts_no_lag_limit[1], \"\\n\")\n",
    "\n",
    "print(\"ADF Statistic for the ts decomposed from seasonal data (with naive method): \", adf_ts_no_season_no_lag_limit[0])\n",
    "print(\"p-value for the ts decomposed from seasonal data (with naive method): \", adf_ts_no_season_no_lag_limit[1], \"\\n\")\n",
    "\n",
    "print(\"critical value (for 5%):\", adf_ts_no_lag_limit[4]['5%'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952e722e-d492-4a9e-bffb-5284a74893bb",
   "metadata": {},
   "source": [
    "Observing the results, it appears as both series indeed are stationary.\n",
    " Also, considering the amount of data we have (235 observations), using more lags would decrease a lot the degrees of freedom in the test and the test could give very misleading results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beeb001-8363-4ce3-9b6e-8dd2c84a2832",
   "metadata": {},
   "source": [
    "**AR MODEL TEST**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba39b2d-4083-4731-bfec-1e7d96253c3a",
   "metadata": {},
   "source": [
    "We are assuming that there is no seasonality in place, because AR model generally does not perform well with non-stationary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295220d2-18cc-43bc-9a67-9f68b2564ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "\n",
    "# As mentioned in previous tests, we will try using the Autoregressive model.\n",
    "# We will try AR(5),AR(6),AR(8),AR(9),AR(10),AR(12),AR(66),AR(83)\n",
    "# AR(66) and AR(83) will not be used because they most likely lead to overfitting\n",
    "\n",
    "ts_model_ar5 = AutoReg(ts.Demand, lags=5).fit()\n",
    "ts_model_ar6 = AutoReg(ts.Demand, lags=6).fit()\n",
    "ts_model_ar8 = AutoReg(ts.Demand, lags=8).fit()\n",
    "ts_model_ar9 = AutoReg(ts.Demand, lags=9).fit()\n",
    "ts_model_ar10 = AutoReg(ts.Demand, lags=10).fit()\n",
    "ts_model_ar12 = AutoReg(ts.Demand, lags=12).fit()\n",
    "\n",
    "pred_ts_ar5 = ts_model_ar5.predict(start=len(ts), end=len(time_serie)-1, dynamic=False)\n",
    "pred_ts_ar6 = ts_model_ar6.predict(start=len(ts), end=len(time_serie)-1, dynamic=False)\n",
    "pred_ts_ar8 = ts_model_ar8.predict(start=len(ts), end=len(time_serie)-1, dynamic=False)\n",
    "pred_ts_ar9 = ts_model_ar9.predict(start=len(ts), end=len(time_serie)-1, dynamic=False)\n",
    "pred_ts_ar10 = ts_model_ar10.predict(start=len(ts), end=len(time_serie)-1, dynamic=False)\n",
    "pred_ts_ar12 = ts_model_ar12.predict(start=len(ts), end=len(time_serie)-1, dynamic=False)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 8)\n",
    "plt.subplot(4, 2, 1)\n",
    "plt.plot(pred_ts_ar5)\n",
    "plt.plot(ts_test.Demand)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"AR(5)\")\n",
    "\n",
    "plt.subplot(4, 2, 2)\n",
    "plt.plot(pred_ts_ar6)\n",
    "plt.plot(ts_test.Demand)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"AR(6)\")\n",
    "\n",
    "plt.subplot(4, 2, 3)\n",
    "plt.plot(pred_ts_ar8)\n",
    "plt.plot(ts_test.Demand)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"AR(8)\")\n",
    "\n",
    "plt.subplot(4, 2, 4)\n",
    "plt.plot(pred_ts_ar9)\n",
    "plt.plot(ts_test.Demand)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"AR(9)\")\n",
    "\n",
    "plt.subplot(4, 2, 5)\n",
    "plt.plot(pred_ts_ar10)\n",
    "plt.plot(ts_test.Demand)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"AR(10)\")\n",
    "\n",
    "plt.subplot(4, 2, 6)\n",
    "plt.plot(pred_ts_ar12)\n",
    "plt.plot(ts_test.Demand)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"AR(12)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d321375-4440-4811-bc4e-da5f2988c760",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.metrics import root_mean_squared_error as rmse\n",
    "print('MAE')\n",
    "print(\"AR(5):\", round(mae(ts_test.Demand, pred_ts_ar5), 3))\n",
    "print(\"AR(6):\", round(mae(ts_test.Demand, pred_ts_ar6), 3))\n",
    "print(\"AR(8):\", round(mae(ts_test.Demand, pred_ts_ar8), 3))\n",
    "print(\"AR(9):\", round(mae(ts_test.Demand, pred_ts_ar9), 3))\n",
    "print(\"AR(10):\", round(mae(ts_test.Demand, pred_ts_ar10), 3))\n",
    "print(\"AR(12):\", round(mae(ts_test.Demand, pred_ts_ar12), 3), \"\\n\")\n",
    "\n",
    "print('RMSE')\n",
    "print(\"AR(5)\", round(rmse(ts_test.Demand, pred_ts_ar5), 3))\n",
    "print(\"AR(6):\", round(rmse(ts_test.Demand, pred_ts_ar6), 3))\n",
    "print(\"AR(8):\", round(rmse(ts_test.Demand, pred_ts_ar8), 3))\n",
    "print(\"AR(9):\", round(rmse(ts_test.Demand, pred_ts_ar9), 3))\n",
    "print(\"AR(10):\", round(rmse(ts_test.Demand, pred_ts_ar10), 3))\n",
    "print(\"AR(12):\", round(rmse(ts_test.Demand, pred_ts_ar12), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcef1a73-ff28-49e8-a63c-2b905e04e1b4",
   "metadata": {},
   "source": [
    "Even though the AR model when using a lot of lags it appears to be following the test data a little better, as we can see\n",
    " from the calculated errors they are not the best choices.<br>\n",
    "From the above results our best choices would be AR(10) and AR(12)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e533a069-bb4b-4b20-a3c2-ecd05aa21073",
   "metadata": {},
   "source": [
    "After some tests, I found that the AR(45) model is the one that gives the best prediction with minimum MAE, RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f0a4a3-7199-4ab6-a5c0-84b5999d0924",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_model_ar45 = AutoReg(ts.Demand, lags=45).fit()\n",
    "pred_ts_ar45= ts_model_ar45.predict(start=len(ts), end=len(time_serie)-1, dynamic=False)\n",
    "plt.plot(pred_ts_ar45)\n",
    "plt.plot(ts_test.Demand)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"AR(45)\")\n",
    "plt.show()\n",
    "print(\"MAE AR(45):\", round(mae(ts_test.Demand, pred_ts_ar45), 3))\n",
    "print(\"RMSE AR(45)\", round(rmse(ts_test.Demand, pred_ts_ar45), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9874bcd8-8eb3-4a20-acc2-c8ed60e21928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the residuals/error term for the AR(45) model we created\n",
    "plt.plot(ts_model_ar45.resid, 'o')\n",
    "plt.show()\n",
    "print(\"Residuals mean: \", round(ts_model_ar45.resid.mean(), 3))\n",
    "print(\"Residuals variance: \", round(ts_model_ar45.resid.var(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbda48c-3a94-4595-abac-694d13079dfc",
   "metadata": {},
   "source": [
    "We can see that the residuls do have a mean of almost zero, but they are not gathered around zero. They are scattered all\n",
    "over the place. This is a strong indication that the AR(45) model is performing poorly.\n",
    "Also, we must have in mind that the more lags we add, the more chance there is for overfitting, and the more we are fitting our model to our specific data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2ea6e8-8af5-41a9-86b5-a9f9fe31be86",
   "metadata": {},
   "source": [
    "**ARMA MODEL TEST**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a4b232-85df-4380-afb2-89baf908836a",
   "metadata": {},
   "source": [
    "We are assuming that there is no seasonality in place, because AR model generally does not perform well with non-stationary data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4ddc23-b45e-49aa-a611-e04f684b7fe7",
   "metadata": {},
   "source": [
    "Again we take a look at the ACF and PACF tests for our time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678afeef-b489-4b80-97eb-187934478a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.graphics.tsaplots as sgt\n",
    "\n",
    "sgt.plot_acf(ts.Demand, lags = 84, zero = False)\n",
    "plt.title(\"ACF Test\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "plt.grid()\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 8)\n",
    "plt.show()\n",
    "\n",
    "sgt.plot_pacf(ts.Demand, lags = 84, zero = False, method = 'ols')\n",
    "plt.title(\"PACF Test\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "plt.grid()\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa7bfd5-dd41-4a62-9205-90dc75acf954",
   "metadata": {},
   "source": [
    "**Deciding ARMA model parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbda2a5c-8085-43dd-bf82-5bb645dc2e4d",
   "metadata": {},
   "source": [
    "Parameter regarding AR part:\n",
    "We can see from the PACF that at first there is no correlation, then some strong correlation occurs with 4-10 lags, 11 is low and\n",
    "12 is high again. There are some more lags indicating correlation later on (e.g.  20, 36 etc), but we will ignore them as they will probably lead to overfitting.\n",
    "So for the AR part we will use the 4, 5, 6, 7, 8, 9, 10 and 12 lags for our tests.\n",
    "\n",
    "Parameter regarding MA part:\n",
    "We can see from the ACF that is strong correlation with 4, 5, 10, 11 first lags. Again, there are some more lags indicating correlation later on (e.g.  23, 36 etc), but we will ignore them too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc58020e-ca4b-47c1-8d6c-992ec763a9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.metrics import root_mean_squared_error as rmse\n",
    "\n",
    "# AR part parameter range\n",
    "p_par = range(4, 13, 1)\n",
    "# MA part parameter range\n",
    "q_par = (4, 5, 10, 11)\n",
    "dic = dict()\n",
    "count = 0\n",
    "\n",
    "for p in p_par:\n",
    "    for q in q_par:\n",
    "        order = (p, 0, q)\n",
    "        model = ARIMA(ts.Demand, order=order).fit()\n",
    "        pred_model = model.predict(start=len(ts), end=len(time_serie)-1, dynamic=False)\n",
    "        \n",
    "        mae_value = round(mae(ts_test.Demand, pred_model), 3)\n",
    "        rmse_value = round(rmse(ts_test.Demand, pred_model), 3)\n",
    "        x = \"For the ARMA(\" + str(p) + \",\" + str(q) + \") Trained Model's prediction, we have MAE=\" + str(mae_value) + \" and RMSE=\" + str(rmse_value)\n",
    "        dic[count] = x\n",
    "        count = count + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9567d5a3-6ef1-4c3c-9a08-91837055c728",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dic:\n",
    "    print(dic[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fe4149-657f-4d99-9be1-166a8e532558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARMA(6, 11) and ARMA(7, 4) seem to be the best choices with MAE~3.74 and RMSE~5.0\n",
    "\n",
    "order = (6, 0, 11)\n",
    "ar6_ma11 = ARIMA(ts.Demand, order=order).fit()\n",
    "pred_ar6_ma11 = ar6_ma11.predict(start=len(ts), end=len(time_serie)-1, dynamic=False)\n",
    "\n",
    "order = (7, 0, 4)\n",
    "ar7_ma4 = ARIMA(ts.Demand, order=order).fit()\n",
    "pred_ar7_ma4 = ar7_ma4.predict(start=len(ts), end=len(time_serie)-1, dynamic=False)\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(pred_ar6_ma11)\n",
    "plt.plot(ts_test.Demand)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"ARMA(6, 11)\")\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(pred_ar7_ma4)\n",
    "plt.plot(ts_test.Demand)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"ARMA(7, 4)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eea23f7-a968-4db7-8dfc-d19413f7a8a1",
   "metadata": {},
   "source": [
    "The above warnings could be generated for various reasons. One of the most common is data being non-stationary.\n",
    "\n",
    "After ARMA model we will try the ARIMA model, as well as the SARIMA model, which we hope to address the (potential) seasonality issue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1ffec2-c510-43d4-99d7-90f20f74e14d",
   "metadata": {},
   "source": [
    "**ARIMA MODEL TEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedddaca-051a-4b37-bd0d-98d8bb4f8aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying different integration levels for AR param=6, 7 and MA param=11, 4\n",
    "\n",
    "dic = dict()\n",
    "\n",
    "order = (6, 1, 11)\n",
    "ar6_d1_ma11 = ARIMA(ts.Demand, order=order).fit()\n",
    "pred_ar6_d1_ma11 = ar6_d1_ma11.predict(start=len(ts), end=len(time_serie)-1, dynamic=False)\n",
    "mae_value = mae(ts_test.Demand, pred_ar6_d1_ma11)\n",
    "rmse_value = rmse(ts_test.Demand, pred_ar6_d1_ma11)\n",
    "dic[0] = \"For the ARIMA\" + str(order) + \" Trained Model's prediction we have MAE=\" + str(mae_value) + \" and RMSE=\" + str(rmse_value)\n",
    "\n",
    "order = (6, 2, 11)\n",
    "ar6_d2_ma11 = ARIMA(ts.Demand, order=order).fit()\n",
    "pred_ar6_d2_ma11 = ar6_d2_ma11.predict(start=len(ts), end=len(time_serie)-1, dynamic=False)\n",
    "mae_value = mae(ts_test.Demand, pred_ar6_d2_ma11)\n",
    "rmse_value = rmse(ts_test.Demand, pred_ar6_d2_ma11)\n",
    "dic[1] = \"For the ARIMA\" + str(order) + \" Trained Model's prediction we have MAE=\" + str(mae_value) + \" and RMSE=\" + str(rmse_value)\n",
    "\n",
    "order = (7, 1, 4)\n",
    "ar7_d1_ma4 = ARIMA(ts.Demand, order=order).fit()\n",
    "pred_ar7_d1_ma4 = ar7_d1_ma4.predict(start=len(ts), end=len(time_serie)-1, dynamic=False)\n",
    "mae_value = mae(ts_test.Demand, pred_ar7_d1_ma4)\n",
    "rmse_value = rmse(ts_test.Demand, pred_ar7_d1_ma4)\n",
    "dic[2] = \"For the ARIMA\" + str(order) + \" Trained Model's prediction we have MAE=\" + str(mae_value) + \" and RMSE=\" + str(rmse_value)\n",
    "\n",
    "order = (7, 2, 4)\n",
    "ar7_d2_ma4 = ARIMA(ts.Demand, order=order).fit()\n",
    "pred_ar7_d2_ma4 = ar7_d2_ma4.predict(start=len(ts), end=len(time_serie)-1, dynamic=False)\n",
    "mae_value = mae(ts_test.Demand, pred_ar7_d2_ma4)\n",
    "rmse_value = rmse(ts_test.Demand, pred_ar7_d2_ma4)\n",
    "dic[3] = \"For the ARIMA\" + str(order) + \" Trained Model's prediction we have MAE=\" + str(mae_value) + \" and RMSE=\" + str(rmse_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d519210-eedc-48f9-a384-bea151b7e465",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dic:\n",
    "    print(dic[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d243ee-d74a-4c31-bbd6-83164ee3fd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best ARIMA model seems to be the ARIMA(7, 1, 4)\n",
    "\n",
    "plt.plot(pred_ar7_d1_ma4)\n",
    "plt.plot(ts_test.Demand)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"ARIMA(7, 1, 4)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44beedc-4b14-4779-85ab-4a58b1e76631",
   "metadata": {},
   "source": [
    "We actually did not expect the ARIMA model to perform better than ARMA model, as ARIMA offers to address trend issues,\n",
    "and there is no kind of trend to be addressed in our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ec0bad-e00d-49cc-bfa7-4f9dfac706a3",
   "metadata": {},
   "source": [
    "**SARIMA MODEL TEST**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe5b6d6-7923-4dfc-83f3-389033f96c9c",
   "metadata": {},
   "source": [
    "In case seasonality exists and plays important role in our time series, then we hope SARIMA model will take this into account\n",
    "and will give us better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5897e09e-c07d-4ce4-a0fc-ef731e3d3da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima import auto_arima\n",
    "\n",
    "# Trying to get best for our model with auto_arima function\n",
    "# m = 12 because the seasonality we expect is daily, which is every 12 observations (1 observation/ 2 hours)\n",
    "model = auto_arima(y = ts.Demand, m = 12)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b20d98-76f7-4d4d-945f-9959c0a44b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best SARIMA model fit found to be ARIMA(0,0,0)(2,0,0)[12]\")\n",
    "\n",
    "sarima_model = ARIMA(ts.Demand, order=(0,0,0), seasonal_order=(2,0,0,12)).fit()\n",
    "pred_sarima = sarima_model.predict(start=len(ts), end=len(time_serie)-1, dynamic=False)\n",
    "mae_value = mae(ts_test.Demand, pred_sarima)\n",
    "rmse_value = rmse(ts_test.Demand, pred_sarima)\n",
    "print(\"For the ARIMA(0,0,0)(2,0,0)[12] Model's prediction we have MAE=\", str(mae_value), \" and RMSE=\", str(rmse_value))\n",
    "plt.plot(pred_sarima)\n",
    "plt.plot(ts_test.Demand)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd26427a-6b88-445d-bec7-530a0ff9a481",
   "metadata": {},
   "source": [
    "Lets try different SARIMA parameters to see if we will have better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df23164-5352-4511-9cf3-565b34b3a1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sar10_ima12 = ARIMA(ts.Demand, order=(10,0,12), seasonal_order=(2,0,0,12)).fit()\n",
    "pred_sar10_ima12 = sar10_ima12.predict(start=len(ts), end=len(time_serie)-1, dynamic=False)\n",
    "mae_value = mae(ts_test.Demand, pred_sar10_ima12)\n",
    "rmse_value = rmse(ts_test.Demand, pred_sar10_ima12)\n",
    "print(\"For the ARIMA(10,0,12)(2,0,0)[12] Model's prediction we have MAE=\", str(mae_value), \" and RMSE=\", str(rmse_value))\n",
    "\n",
    "sar6_ima11 = ARIMA(ts.Demand, order=(6,0,11), seasonal_order=(2,0,0,12)).fit()\n",
    "pred_sar6_ima11 = sar6_ima11.predict(start=len(ts), end=len(time_serie)-1, dynamic=False)\n",
    "mae_value = mae(ts_test.Demand, pred_sar6_ima11)\n",
    "rmse_value = rmse(ts_test.Demand, pred_sar6_ima11)\n",
    "print(\"For the ARIMA(6,0,11)(2,0,0)[12] Model's prediction we have MAE=\", str(mae_value), \" and RMSE=\", str(rmse_value))\n",
    "\n",
    "sar7_ima4 = ARIMA(ts.Demand, order=(7,0,4), seasonal_order=(2,0,0,12)).fit()\n",
    "pred_sar7_ima4 = sar7_ima4.predict(start=len(ts), end=len(time_serie)-1, dynamic=False)\n",
    "mae_value = mae(ts_test.Demand, pred_sar7_ima4)\n",
    "rmse_value = rmse(ts_test.Demand, pred_sar7_ima4)\n",
    "print(\"For the ARIMA(7,0,4)(2,0,0)[12] Model's prediction we have MAE=\", str(mae_value), \" and RMSE=\", str(rmse_value))\n",
    "\n",
    "sar10_ima1 = ARIMA(ts.Demand, order=(10,0,1), seasonal_order=(2,0,0,12)).fit()\n",
    "pred_sar10_ima1 = sar10_ima1.predict(start=len(ts), end=len(time_serie)-1, dynamic=False)\n",
    "mae_value = mae(ts_test.Demand, pred_sar10_ima1)\n",
    "rmse_value = rmse(ts_test.Demand, pred_sar10_ima1)\n",
    "print(\"For the ARIMA(10,0,1)(2,0,0)[12] Model's prediction we have MAE=\", str(mae_value), \" and RMSE=\", str(rmse_value))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(pred_sar10_ima12)\n",
    "plt.plot(ts_test.Demand)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"ARIMA(10,0,12)(2,0,0)[12]\")\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(pred_sar6_ima11)\n",
    "plt.plot(ts_test.Demand)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"ARIMA(6,0,11)(2,0,0)[12]\")\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(pred_sar7_ima4)\n",
    "plt.plot(ts_test.Demand)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"ARIMA(7,0,4)(2,0,0)[12]\")\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(pred_sar10_ima1)\n",
    "plt.plot(ts_test.Demand)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"ARIMA(10,0,1)(2,0,0)[12]\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00994969-c9d5-4e55-8aaa-4a19f2cb210b",
   "metadata": {},
   "source": [
    "Lets test the ARIMA(7,0,4)(2,0,0)[12], but with different integration levels to see if it will result in better prediciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647477b7-9e96-4d3e-962e-a7a9143188a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sar7_i1_ma4 = ARIMA(ts.Demand, order=(7,1,4), seasonal_order=(2,0,0,12)).fit()\n",
    "pred_sar7_i1_ma4 = sar7_i1_ma4.predict(start=len(ts), end=len(time_serie)-1, dynamic=False)\n",
    "mae_value = mae(ts_test.Demand, pred_sar7_i1_ma4)\n",
    "rmse_value = rmse(ts_test.Demand, pred_sar7_i1_ma4)\n",
    "print(\"For the ARIMA(7,1,4)(2,0,0)[12] Model's prediction we have MAE=\", str(mae_value), \" and RMSE=\", str(rmse_value))\n",
    "\n",
    "sar7_i2_ma4 = ARIMA(ts.Demand, order=(7,2,4), seasonal_order=(2,0,0,12)).fit()\n",
    "pred_sar7_i2_ma4 = sar7_i2_ma4.predict(start=len(ts), end=len(time_serie)-1, dynamic=False)\n",
    "mae_value = mae(ts_test.Demand, pred_sar7_i2_ma4)\n",
    "rmse_value = rmse(ts_test.Demand, pred_sar7_i2_ma4)\n",
    "print(\"For the ARIMA(7,2,4)(2,0,0)[12] Model's prediction we have MAE=\", str(mae_value), \" and RMSE=\", str(rmse_value))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(pred_sar7_i1_ma4)\n",
    "plt.plot(ts_test.Demand)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"ARIMA(7,2,4)(2,0,0)[12]\")\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(pred_sar7_i2_ma4)\n",
    "plt.plot(ts_test.Demand)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"ARIMA(7,2,4)(2,0,0)[12]\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
