{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5ea0d3-cc1c-41fc-a1c3-896219369a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################## GENERAL IDEA ###################################################################\n",
    "###################################################################################################################################################\n",
    "############################################################### 1. PRE PROCESSING #################################################################\n",
    "###################################################################################################################################################\n",
    "## Create a data frame from csv file, then create two data frames one for each Station types (start, end) for easier data handling. \n",
    "## Making sure data are sorted by datetimes, handling missing data (if any), getting rid of data we do not care about, converting certain columns \n",
    "## to certain data types if needed.\n",
    "## Then, create intervals and calculate values for each interval, handle upper/lower interval problems, and finally creating the final time_serie, \n",
    "## handling missing values if necessary. \n",
    "## All of the above are being conducted inside a function wich returns the final time series. \n",
    "## I did this because I wanted to easily create different time series using different intervals for testing purposes.\n",
    "## At the end of pre processing, we devide the time series into training and testing data. This is also being conducted in a function because\n",
    "## I wanted to easily create different training/testing data analogies for testing purposes.\n",
    "###################################################################################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d9b842-5da1-4e7f-a9cb-80049888c34b",
   "metadata": {},
   "source": [
    "**PREPROCESSING OF DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477e91d4-086c-416f-b81d-e931cd6391c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### Time series creating function, using a file name and an integer representing the interval in hours ###########################\n",
    "\n",
    "# Importing pandas library\n",
    "import pandas as pd\n",
    "\n",
    "def csvDataToTimeSerie(fileName, timeInterInHours):  \n",
    "    # Creating a pandas data frame with the raw data of the csv file\n",
    "    raw_df = pd.read_csv(fileName)\n",
    "\n",
    "    # Creating separate data frames for start and end stations\n",
    "    \n",
    "    raw_start_df = raw_df[['TripId', 'StartTime', 'StartStationId']]\n",
    "    raw_end_df = raw_df[['TripId', 'EndTime', 'EndStationId']]\n",
    "    \n",
    "    # Making sure the data we are going to work on are sorted by timestamp\n",
    "    \n",
    "    raw_start_sorted_df = raw_start_df.sort_values(by='StartTime')\n",
    "    raw_end_sorted_df = raw_end_df.sort_values(by='EndTime')\n",
    "    \n",
    "    # Keeping only data that we want in each data frame (deleting entries where station id is not equal to 1)\n",
    "    # Also resetting the data frames' indexes for better visualization-manipulation of data\n",
    "    \n",
    "    condition = raw_start_sorted_df['StartStationId'] == 1\n",
    "    raw_start_sorted_df = raw_start_sorted_df[condition]\n",
    "    raw_start_sorted_df = raw_start_sorted_df.reset_index(drop='True')\n",
    "    \n",
    "    condition = raw_end_sorted_df['EndStationId'] == 1\n",
    "    raw_end_sorted_df = raw_end_sorted_df[condition]\n",
    "    raw_end_sorted_df = raw_end_sorted_df.reset_index(drop='True')\n",
    "    \n",
    "    # Checking if there are NaN values in the the raw data frames\n",
    "    \n",
    "    start_df_nan_check = raw_start_sorted_df.isna().any()\n",
    "    end_df_nan_check = raw_end_sorted_df.isna().any()\n",
    "    # In this project there are non. If there where, we would have to decide how to deal with the empty spots\n",
    "    \n",
    "    # Converting Time columns to timestamps so that certain functions can be used on them\n",
    "    \n",
    "    raw_start_sorted_df['StartTime'] = pd.to_datetime(raw_start_sorted_df['StartTime'])\n",
    "    raw_end_sorted_df['EndTime'] = pd.to_datetime(raw_end_sorted_df['EndTime'])\n",
    "\n",
    "    # Defining the start and end timestamps of the time series\n",
    "    # First we compare the min and max timestamps from the two data frames\n",
    "    # and we keep the overall min and max timestamps\n",
    "    \n",
    "    start_time_1 = raw_start_sorted_df['StartTime'].min()\n",
    "    start_time_2 = raw_end_sorted_df['EndTime'].min()\n",
    "    if (start_time_1 > start_time_2):\n",
    "        start_time = start_time_2\n",
    "    else:\n",
    "        start_time = start_time_1\n",
    "    \n",
    "    end_time_1 = raw_start_sorted_df['StartTime'].max()\n",
    "    end_time_2 = raw_end_sorted_df['EndTime'].max()\n",
    "    if (end_time_1 > end_time_2):\n",
    "        end_time = end_time_1\n",
    "    else:\n",
    "        end_time = end_time_2\n",
    "    \n",
    "    # Then we round the starting and ending hour (to the floor and ceiling hours respectively)\n",
    "    \n",
    "    start_time = start_time.floor('h')\n",
    "    end_time = end_time.ceil('h')\n",
    "    \n",
    "    # This if will take action only if the timeInterInHours exactly devides 24 (total hours per day)\n",
    "    # Not necessary but if the above is the case, then intervals deviding the total period will not split between different days\n",
    "    \n",
    "    if 24 % timeInterInHours == 0:\n",
    "        start_mod = start_time.hour % timeInterInHours\n",
    "        if start_mod != 0:\n",
    "            start_time -= pd.Timedelta(hours=start_mod)\n",
    "        \n",
    "        end_mod = end_time.hour % timeInterInHours\n",
    "        if end_mod != 0:\n",
    "            end_time += pd.Timedelta(hours=end_mod)\n",
    "    \n",
    "    # Creating the final timestamp values of the time series using a frequency of timeInterInHours hours\n",
    "    # and inserting it into a pandas data frame\n",
    "    # Each timestamp of the time_serie will indicate the demand for the time period [current_timestamp, current_timestamp+Xhours)\n",
    "    \n",
    "    interval = str(timeInterInHours) + 'h'\n",
    "    timestamps = pd.date_range(start=start_time, end=end_time, freq=interval)\n",
    "    \n",
    "    # There is a change that the max timestamp created is less than the end_time of our data.\n",
    "    # If this is the case we add one more interval, so that we will not miss any data when creating the time serie\n",
    "    \n",
    "    if (timestamps[-1] < end_time):\n",
    "        extra_timestamp = timestamps[-1] + pd.Timedelta(hours=timeInterInHours)\n",
    "        timestamps = timestamps.append(pd.DatetimeIndex([extra_timestamp]))\n",
    "    \n",
    "    # Creating the total period for the time serie\n",
    "    \n",
    "    pre_time_serie = pd.DataFrame({'Time': timestamps})\n",
    "    \n",
    "    # Calculating the bicycles taken and returned in each period,\n",
    "    # and then calculating the demand in ech period by subtracting the above\n",
    "    \n",
    "    pre_time_serie['Taken'] = 0\n",
    "    pre_time_serie['Returned'] = 0\n",
    "    \n",
    "    # Inserting the taken bicycle values\n",
    "    i = 0\n",
    "    max_i = raw_start_sorted_df.index.max()\n",
    "    max_index = pre_time_serie.index.max()\n",
    "    for index in range(0, max_index):\n",
    "        count = 0\n",
    "        cur_time = raw_start_sorted_df.loc[i, 'StartTime']\n",
    "        low_time = pre_time_serie.loc[index, 'Time']\n",
    "        high_time = pre_time_serie.loc[index+1, 'Time']\n",
    "        while (cur_time>=low_time and cur_time<high_time):\n",
    "            count += 1\n",
    "            i += 1\n",
    "            if (i>max_i): break\n",
    "            cur_time = raw_start_sorted_df.loc[i, 'StartTime']\n",
    "        pre_time_serie.loc[index, 'Taken'] = count\n",
    "        if (i>max_i): break\n",
    "    \n",
    "    # Inserting the returned bicycle values\n",
    "    i = 0\n",
    "    max_i = raw_end_sorted_df.index.max()\n",
    "    for index in range(0, max_index):\n",
    "        count = 0\n",
    "        cur_time = raw_end_sorted_df.loc[i, 'EndTime']\n",
    "        low_time = pre_time_serie.loc[index, 'Time']\n",
    "        high_time = pre_time_serie.loc[index+1, 'Time']\n",
    "        while (cur_time>=low_time and cur_time<high_time):\n",
    "            count += 1\n",
    "            i += 1\n",
    "            if (i>max_i): break\n",
    "            cur_time = raw_end_sorted_df.loc[i, 'EndTime']\n",
    "        pre_time_serie.loc[index, 'Returned'] = count\n",
    "        if (i>max_i): break\n",
    "    \n",
    "    # Creating the resulting time serie by deleted columns that are not needed,\n",
    "    # deleting the last row, which does not represent a period we want\n",
    "    # and make the timestamps' column the index\n",
    "    # We do not have to handle missing values etc,\n",
    "    # because as we can see from the raw data set, data is spread in any kind day/hour\n",
    "    \n",
    "    # If we had to deal with missing values, then usually one of three methods is chosen:\n",
    "    # 1. Fill with front interval's value with time_serie.Demand.fillna(method = 'bfill') a.k.a \"back filling\"\n",
    "    # 2. Fill with back interval's value with time_serie.Demand.fillna(method = 'ffill') a.k.a \"front filling\"\n",
    "    # 3. Fill with the mean of all time_serie data with time_serie.Demand.fillna(value = time_serie.Demand.mean()) -- NOT good choice for time series\n",
    "    #    because usually we observe a periodicity in data and data are strongly connected with neighbor data\n",
    "    \n",
    "    pre_time_serie['Demand'] = pre_time_serie['Returned'] - pre_time_serie['Taken']\n",
    "    time_serie = pre_time_serie.copy()\n",
    "    time_serie.drop(columns=['Returned'], inplace=True)\n",
    "    time_serie.drop(columns=['Taken'], inplace=True)\n",
    "    time_serie.drop(time_serie.index[-1], inplace=True)\n",
    "    time_serie.set_index('Time', inplace=True)\n",
    "    return time_serie\n",
    "\n",
    "############################################ Function to split the time series into training and testing data #######################################\n",
    "\n",
    "def timeSerieSplit(time_serie, training_perc, testing_perc):\n",
    "    size = int(len(time_serie)*training_perc)\n",
    "    training_data = time_serie.iloc[:size]\n",
    "    testing_data = time_serie.iloc[size:]\n",
    "    return training_data, testing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb39822-0db4-4c0a-b51f-207755ee3ee8",
   "metadata": {},
   "source": [
    "**TIME SERIES CREATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30d5079-3ec0-430e-8b17-96e453a9dd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a time series from the data.csv file's data, with a 2 hour interval and splitting into 70% training, 30% testing data ###\n",
    "\n",
    "time_serie = csvDataToTimeSerie('data.csv', 2)\n",
    "ts, ts_test = timeSerieSplit(time_serie, 0.7, 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7960ab6a-dd73-48e6-a200-034df2777fa9",
   "metadata": {},
   "source": [
    "**TIME SERIES ANALYSIS START**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e438a5e-1b70-4705-b5e1-2f2ef53ae117",
   "metadata": {},
   "source": [
    "**COMPARING OUR TIME SERIES WITH WHITE NOISE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c323a2e5-35c2-4049-a689-7fe35f58011b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the time serie through the data frame\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "ts.Demand.plot(figsize = (20, 8))\n",
    "plt.title(\"Bicycle Demand\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Demand\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# White noise plot, following a normal distribution with mean and std derived from the time series data\n",
    "white_noise = np.random.normal(loc = ts.Demand.mean(), scale = ts.Demand.std(), size = len(ts))\n",
    "ts.loc[:,'white_noise'] = white_noise\n",
    "print(ts)\n",
    "ts.white_noise.plot(figsize = (20, 8))\n",
    "plt.title('White Noise TS')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('White Noise')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# We can see that the bicycle demand time series plot looks a lot like the white noise time series plot\n",
    "# The surely both do not have any trend, having a mean very close to zero. The std also seems to be constant.\n",
    "# Seasonality and auto-correlation is the things we observe that could exists in our time serie, thus making it non stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d238c9-6545-4a45-b7c5-328d49e3fe90",
   "metadata": {},
   "source": [
    "**FINDING TREND**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f743e3e-aa0f-410b-a410-315b2e594719",
   "metadata": {},
   "source": [
    "It is obvious by the previous graph that no trend exists in the time series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd290659-98bf-468b-8e51-1eb945e11625",
   "metadata": {},
   "source": [
    "**FINDIND SEASONALITY**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0af05a-49f4-4202-b80f-670d45f43976",
   "metadata": {},
   "source": [
    "**CORRELATION TEST WITH ACF METHOD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df924a6-4f0f-48a1-90db-0964ec7aee85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import statsmodels.graphics.tsaplots as sgt\n",
    "sgt.plot_acf(ts.Demand, lags = 83, zero = False)\n",
    "plt.title(\"Autocorrelation in max 1 week difference\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "plt.grid()\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3870812-83b9-405f-92ea-b7c34962ea58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even though the ACF method does not give strong direct/indirect correlations between present and past values,\n",
    "# we can see a pattern in the graph with a frequency of 1 day (12 lags x 2 hour interval)\n",
    "# This indicates that a periodicity exists, which we should remove before we proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d1179f-8f90-44a7-bc7d-523c0b6ef7bc",
   "metadata": {},
   "source": [
    "**CORRELATION TEST WITH PACF METHOD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ec0f51-3478-453f-82d1-5ab0b0d3706b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgt.plot_pacf(ts.Demand, lags = 83, zero = False, method = 'ols')\n",
    "plt.title(\"Autocorrelation in max 1 week difference\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Correlation\")\n",
    "plt.grid()\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66748aed-36ee-4c21-9436-fe0a634b2787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the use of PACF method we conclude that there is no significant direct correlation between timestamps of any lag size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c51e81-58b5-4057-bfab-c25ae0b4417a",
   "metadata": {},
   "source": [
    "**DECOMPOSING SEASONALITY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec6f9b3-e6c3-451a-b209-aa272622430f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We will use seasonal decomposition to split the seasonality from the time series\n",
    "\n",
    "import statsmodels.tsa.seasonal as stse\n",
    "\n",
    "s_dec_add = stse.seasonal_decompose(ts.Demand)\n",
    "s_dec_add.plot()\n",
    "plt.xticks(rotation = 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b61c2f-737f-4f45-8746-9785e4ac9ef1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ts_seasonality = s_dec_add.seasonal\n",
    "ts_seasonality = ts_seasonality.to_frame()\n",
    "ts_seasonality = ts_seasonality.rename(columns={'seasonal': 'season_demand'})\n",
    "ts_no_season = ts.copy()\n",
    "ts_no_season['Demand'] = ts['Demand'] - ts_seasonality['season_demand']\n",
    "\n",
    "ts.Demand.plot()\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "plt.title(\"Original Time Series\")\n",
    "plt.show()\n",
    "ts_seasonality.season_demand.plot()\n",
    "plt.title(\"Seasonality in Time Series\")\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "plt.show()\n",
    "ts_no_season.Demand.plot()\n",
    "plt.title(\"Residuals in Time Series (No Seasonality, No Trend)\")\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57fb08e-4e66-4ddc-adf5-0944e9da1eb9",
   "metadata": {},
   "source": [
    "**STATIONARITY TEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca24d5db-1232-49de-90dd-a81b6e6e1606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.tsa.stattools as sts\n",
    "\n",
    "print (sts.adfuller(ts_no_season.Demand, maxlag = 83))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20572f56-63ea-4c55-91ad-e8026b157b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We observe that the ADF_statistic < critical_value (-4.03 < -2.8) and that the possibility of the ts_no_season being stationary is 0.001%.\n",
    "# So, it is safe to declare that our time series free from its seasonality is stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beeb001-8363-4ce3-9b6e-8dd2c84a2832",
   "metadata": {},
   "source": [
    "**AR MODEL TEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295220d2-18cc-43bc-9a67-9f68b2564ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "\n",
    "# Since we found a seasonality of 1 day (12 intervals),\n",
    "# then we will try to predict our test data using multiplications of 12 lags for the AR Model creation \n",
    "\n",
    "model = AutoReg(ts.Demand, lags=12).fit()\n",
    "prediction = model.predict(start=len(ts), end=len(time_serie), dynamic=False)\n",
    "\n",
    "plt.plot(prediction, color=\"red\")\n",
    "plt.plot(ts_test.Demand, color=\"blue\")\n",
    "plt.title(\"Prediction vs Test Data\")\n",
    "plt.show()\n",
    "\n",
    "model = AutoReg(ts.Demand, lags=87).fit()\n",
    "prediction = model.predict(start=len(ts), end=len(time_serie), dynamic=False)\n",
    "plt.plot(prediction, color=\"red\")\n",
    "\n",
    "plt.plot(ts_test.Demand, color=\"blue\")\n",
    "plt.title(\"Prediction vs Test Data\")\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b48640f-b7da-40b1-97e5-ed95658098e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is obvious that for lower values for the number of lags the predictions are way off.\n",
    "# When trying with more lags, the predictions seem to be a lot better.\n",
    "# There are many points where the predictions are way off though, so we will try different models too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdaa9a3-3eaa-45f3-8339-121bb30c8582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as sc\n",
    "import pylab\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 8)\n",
    "sc.probplot(time_serie.Demand, plot = pylab)\n",
    "pylab.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
